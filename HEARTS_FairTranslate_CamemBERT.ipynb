{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cec70/HEARTS-French-Adaptation/blob/main/HEARTS_FairTranslate_CamemBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da46830",
      "metadata": {
        "id": "0da46830"
      },
      "source": [
        "# French Stereotype Detection with CamemBERT\n",
        "\n",
        "This notebook adapts the HEARTS framework to the French language, focusing on occupational gender stereotypes in translations.\n",
        "\n",
        "Instead of the EMGSD dataset used in HEARTS, the chosen dataset is FairTranslate, a curated English–French dataset designed to evaluate gender expression in machine translation.\n",
        "\n",
        "The goals of this project are to:\n",
        "- Adapt the HEARTS methodology to a new linguistic and cultural context\n",
        "- Fine-tune a French-specific language model (CamemBERT) for stereotype detection\n",
        "- Compare its performance to a TF-IDF baseline and to the original HEARTS results\n",
        "- Evaluate sustainability impacts and ensure reproducible training\n",
        "- Conduct statistical significance testing and qualitative error analysis\n",
        "\n",
        "**Notes before running:**\n",
        "- This notebook expects to fetch the FairTranslate dataset from Hugging Face.\n",
        "- Training Transformer models requires a GPU for practical speed.\n",
        "\n",
        "Run cells in order."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac95689c",
      "metadata": {
        "id": "ac95689c"
      },
      "source": [
        "## Contextually relevant challenge\n",
        "\n",
        "Gender stereotypes in the French language are reinforced through grammatical gender and the lack of standardisation around inclusive writing.\n",
        "\n",
        "These linguistic patterns can amplify existing social biases when used in translation systems or automated text generation.\n",
        "\n",
        "#### SDG alignment\n",
        "This challenge connects with multiple SDGs:\n",
        "- **SDG 5 (Gender Equality)**: addressing representational bias in language technologies\n",
        "- **SDG 10 (Reduced Inequalities)**: mitigating discriminatory gender associations\n",
        "- **SDG 8 (Decent Work)**: avoiding biased occupational representations that can reinforce an unequal job market\n",
        "\n",
        "#### Limitations and ethical considerations\n",
        "- Occupational stereotypes embedded in language carry real consequences for gendered representation.\n",
        "- Inclusive forms like “infirmier.ère” or “enseignant.e” lack standardisation, which can make them difficult for models to learn.\n",
        "- Risk of reinforcing harmful stereotypes if detection systems are misinterpreted as prescriptive rather than diagnostic.\n",
        "- Dataset size is small (2.4k examples), which limits robustness.\n",
        "\n",
        "#### Scalability and sustainability\n",
        "- The French NLP ecosystem is smaller than English so scalability depends on the availability of high-quality data.\n",
        "- Using CamemBERT (110M parameters) is more sustainable than larger French language models.\n",
        "- Emissions are monitored with CodeCarbon to evaluate energy footprint and align with sustainable AI principles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57bbfa42",
      "metadata": {
        "id": "57bbfa42"
      },
      "source": [
        "## 0. Environment & pip installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46f4f288",
      "metadata": {
        "id": "46f4f288"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "python -V\n",
        "\n",
        "python -m pip install --upgrade pip --quiet\n",
        "python -m pip install transformers datasets scikit-learn pandas numpy torch torchvision torchaudio --quiet\n",
        "python -m pip install accelerate evaluate codecarbon sentencepiece sacremoses --quiet\n",
        "python -m pip install git+https://github.com/huggingface/transformers --quiet || true"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b240404d",
      "metadata": {
        "id": "b240404d"
      },
      "source": [
        "## 1. Imports and global configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f98f7b",
      "metadata": {
        "id": "01f98f7b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# MUST be set before importing torch\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"ATTENTION_BACKEND\"] = \"PYTORCH\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "torch.set_float32_matmul_precision(\"highest\")\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "# Paths and config\n",
        "OUTPUT_DIR = 'outputs_camembert_fairtranslate'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Model choices\n",
        "MODEL_NAME = 'camembert-base'  # CamemBERT for French\n",
        "BATCH_SIZE = 64\n",
        "LR = 2e-5\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "print(f\"Using model: {MODEL_NAME}\")\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "815c83cd",
      "metadata": {
        "id": "815c83cd"
      },
      "source": [
        "## 2. Data loading\n",
        "\n",
        "To adapt the HEARTS framework to the French setting we use FairTranslate, a dataset designed for evaluating gender expression in English–French translation systems.\n",
        "\n",
        "#### Why FairTranslate is appropriate\n",
        "- Contains gender-marked occupational terms, which are important to stereotype evaluation in French\n",
        "- Includes three gender variants (male, female, inclusive) enabling counterfactual fairness checks\n",
        "- Includes ambiguity levels (\"ambiguous\", \"unambiguous\", \"long unambiguous\")\n",
        "- Provides context-specific occupational stereotype labels based on European workforce statistics\n",
        "\n",
        "#### Data access and ethics\n",
        "- Accessed openly via HuggingFace, respecting open-data licensing\n",
        "- Contains no personal or sensitive data\n",
        "- No privacy or consent issues as data does not contain identifiable individuals\n",
        "- This dataset better suits the linguistic characteristics of French, making it a valid adaptation of HEARTS to a Francophone context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d24ac8d",
      "metadata": {
        "id": "4d24ac8d"
      },
      "outputs": [],
      "source": [
        "# Load dataset from Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = 'Fannyjrd/FairTranslate_fr'\n",
        "\n",
        "try:\n",
        "    ds = load_dataset(dataset_name)\n",
        "    ds = ds.shuffle(seed=SEED)\n",
        "    print(\"Loaded dataset from Hugging Face\")\n",
        "    # inspect\n",
        "    print(ds)\n",
        "except Exception as e:\n",
        "    print(f\"Could not load from HF (maybe offline): {e}\")\n",
        "\n",
        "# If Hugging Face dataset object, convert to DataFrame for flexible processing\n",
        "if 'train' in ds:\n",
        "    try:\n",
        "        df = ds['train'].to_pandas()\n",
        "    except Exception:\n",
        "        # some HF datasets are dict-like\n",
        "        df = pd.DataFrame(ds['train'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c97ac93",
      "metadata": {
        "id": "4c97ac93"
      },
      "source": [
        "## 3. Preprocessing and label mapping\n",
        "\n",
        "The dataset is mapped to a binary `stereotype` vs `non-stereotype` label to match HEARTS' binary experiments.\n",
        "\n",
        "- 1 (`stereotype`) -> `male-stereotyped` and `female-stereotyped`\n",
        "- 0 (`non-stereotype`) -> `balanced`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6f8051",
      "metadata": {
        "id": "8f6f8051"
      },
      "outputs": [],
      "source": [
        "# Inspect available columns to choose mapping\n",
        "print(f\"Columns available: {df.columns.tolist()}\")\n",
        "\n",
        "# Heuristic mapping (modify depending on exact column names)\n",
        "if 'stereotype' in df.columns:\n",
        "    def map_stereo(x):\n",
        "        if pd.isna(x):\n",
        "            return 0\n",
        "        x = str(x).lower()\n",
        "        if 'stereotype' in x or 'male' in x or 'female' in x:\n",
        "            # treat explicitly male/female stereotyped as stereotype\n",
        "            if 'balanced' in x or 'gender-balanced' in x or 'balanced' in x:\n",
        "                return 0\n",
        "            return 1\n",
        "        return 0\n",
        "    df['label'] = df['stereotype'].apply(map_stereo)\n",
        "else:\n",
        "    # If the dataset already provides a 'label' or 'gender' mapping, adjust here\n",
        "    if 'Gender' in df.columns or 'gender' in df.columns:\n",
        "        # fallback: build a simple heuristic\n",
        "        df['label'] = df.get('stereotype', 0)\n",
        "    else:\n",
        "        # If no metadata present, ask user to inspect\n",
        "        print(\"No obvious stereotype column found. Please inspect df.columns and adapt mapping cell.\")\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "print(df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0IQg8Xe-0IHJ",
      "metadata": {
        "id": "0IQg8Xe-0IHJ"
      },
      "outputs": [],
      "source": [
        "# Dataset visualisations\n",
        "\n",
        "# Distribution of stereotype labels\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x=\"stereotype\")\n",
        "plt.title(\"Distribution of Stereotype Labels\")\n",
        "plt.xlabel(\"Stereotype category\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Gender variant distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x=\"gender\")\n",
        "plt.title(\"Gender Variant Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results\n",
        "\n",
        "Label Distribution:\n",
        "- 1 -> 1716\n",
        "- 0 -> 702\n",
        "\n",
        "Dataset distribution of stereotype labels:\n",
        "- `male-stereotyped`-> 858\n",
        "- `female-stereotyped` -> 858\n",
        "- `gender-balanced` -> 702\n",
        "\n",
        "Gender variant dataset distribution:\n",
        "- `male`-> 806\n",
        "- `female` -> 806\n",
        "- `inclusive` -> 806"
      ],
      "metadata": {
        "id": "vTYbA-8MTgRB"
      },
      "id": "vTYbA-8MTgRB"
    },
    {
      "cell_type": "markdown",
      "id": "b884a3cf",
      "metadata": {
        "id": "b884a3cf"
      },
      "source": [
        "## 4. Train / validation / test split\n",
        "\n",
        "Use stratified split with 80% train, 10% validation, 10% test (or 80/20 train/test with a validation split from training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b73a7e0",
      "metadata": {
        "id": "0b73a7e0"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=SEED)\n",
        "# optionally split train to train+val\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.125, stratify=train_df['label'], random_state=SEED)  # 0.125 * 0.8 = 0.1\n",
        "\n",
        "print(f\"Train: {len(train_df)}\")\n",
        "print(f\"Val: {len(val_df)}\")\n",
        "print(f\"Test: {len(test_df)}\")\n",
        "\n",
        "# Quick save copies for reproducibility\n",
        "train_df.to_csv(os.path.join(OUTPUT_DIR, 'train_split.csv'), index=False)\n",
        "val_df.to_csv(os.path.join(OUTPUT_DIR, 'val_split.csv'), index=False)\n",
        "test_df.to_csv(os.path.join(OUTPUT_DIR, 'test_split.csv'), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results\n",
        "\n",
        "- Train: 1692\n",
        "- Val: 242\n",
        "- Test: 484"
      ],
      "metadata": {
        "id": "Otq8j5L9aWbW"
      },
      "id": "Otq8j5L9aWbW"
    },
    {
      "cell_type": "markdown",
      "id": "fe739fc5",
      "metadata": {
        "id": "fe739fc5"
      },
      "source": [
        "## 5. Baseline: TF-IDF + Logistic Regression\n",
        "\n",
        "Train a simple TF-IDF + LR baseline to compare with CamemBERT's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b61ecd4",
      "metadata": {
        "id": "6b61ecd4"
      },
      "outputs": [],
      "source": [
        "# TF-IDF baseline\n",
        "vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "X_train = vectorizer.fit_transform(train_df['french'].astype(str))\n",
        "X_val = vectorizer.transform(val_df['french'].astype(str))\n",
        "X_test = vectorizer.transform(test_df['french'].astype(str))\n",
        "\n",
        "y_train = train_df['label']\n",
        "y_val = val_df['label']\n",
        "y_test = test_df['label']\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=SEED)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "baseline_preds = clf.predict(X_test)\n",
        "print(\"TF-IDF + LR Test classification report:\")\n",
        "print(classification_report(y_test, baseline_preds, digits=4))\n",
        "print(f\"Macro F1: {f1_score(y_test, baseline_preds, average='macro')}\")\n",
        "\n",
        "# Save baseline\n",
        "import joblib\n",
        "joblib.dump(vectorizer, os.path.join(OUTPUT_DIR, 'tfidf_vectorizer.joblib'))\n",
        "joblib.dump(clf, os.path.join(OUTPUT_DIR, 'tfidf_lr.joblib'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results\n",
        "\n",
        "**Macro F1**: 0.9850506548060292"
      ],
      "metadata": {
        "id": "-hHK3mCwYFqK"
      },
      "id": "-hHK3mCwYFqK"
    },
    {
      "cell_type": "markdown",
      "id": "177b5451",
      "metadata": {
        "id": "177b5451"
      },
      "source": [
        "## 6. Data preparation (tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb46148",
      "metadata": {
        "id": "3fb46148"
      },
      "outputs": [],
      "source": [
        "np.random.seed(SEED)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
        "\n",
        "def tokenize_fn(examples):\n",
        "    tokens = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
        "    tokens['label'] = examples['label']\n",
        "    return tokens\n",
        "\n",
        "# Build HF datasets for Trainer\n",
        "from datasets import Dataset\n",
        "train_ds = Dataset.from_pandas(train_df.rename(columns={'french':'text'})[['text','label']]).shuffle(seed=SEED)\n",
        "val_ds   = Dataset.from_pandas(val_df.rename(columns={'french':'text'})[['text','label']])\n",
        "test_ds  = Dataset.from_pandas(test_df.rename(columns={'french':'text'})[['text','label']])\n",
        "\n",
        "train_tok = train_ds.map(tokenize_fn, batched=True, num_proc=1, load_from_cache_file=False)\n",
        "val_tok   = val_ds.map(tokenize_fn, batched=True, num_proc=1, load_from_cache_file=False)\n",
        "test_tok  = test_ds.map(tokenize_fn, batched=True, num_proc=1, load_from_cache_file=False)\n",
        "\n",
        "train_tok.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
        "val_tok.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
        "test_tok.set_format(type='torch', columns=['input_ids','attention_mask','label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe04fe50",
      "metadata": {
        "id": "fe04fe50"
      },
      "source": [
        "## 7. CamemBERT: model definition and training\n",
        "\n",
        "The HEARTS framework uses **BERT** models for English text.\n",
        "In the French context, this must be adapted for several linguistic reasons:\n",
        "\n",
        "#### Why CamemBERT\n",
        "- CamemBERT is trained on large-scale French data (OSCARS corpus)\n",
        "- Handles grammatical gender and inclusive constructions\n",
        "- French morphology makes gender cues explicit, unlike English, therefore requiring a model trained on French\n",
        "\n",
        "#### Architectural justification\n",
        "- CamemBERT-base (110M parameters) balances accuracy and sustainability\n",
        "- Reduced epochs limit compute cost and emissions\n",
        "\n",
        "#### Hyperparameter tuning\n",
        "\n",
        "- Learning rate: {1e-5, 2e-5, 5e-5}\n",
        "- Batch size: {32, 64}\n",
        "- Epochs: {4, 5, 6}\n",
        "\n",
        "The final configuration was selected based on validation macro-F1 and training stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c87c01f9",
      "metadata": {
        "id": "c87c01f9"
      },
      "outputs": [],
      "source": [
        "# `camembert-base` model is fine-tuned using Hugging Face Trainer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2, attn_implementation=\"eager\", dtype=torch.float32)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LR,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_f1',\n",
        "    greater_is_better=True,\n",
        "    seed=SEED,\n",
        "    data_seed=SEED,\n",
        "    dataloader_num_workers=0,\n",
        "    dataloader_drop_last=False,\n",
        "    torch_compile=False,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    tf32=False,\n",
        "    optim='adamw_torch',\n",
        "    group_by_length=False,\n",
        ")\n",
        "\n",
        "# Compute_metrics for Trainer\n",
        "import evaluate\n",
        "f1 = evaluate.load('f1')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    f1_value = f1.compute(predictions=preds, references=labels, average='macro')[\"f1\"]\n",
        "\n",
        "    return {\n",
        "        'precision': precision_score(labels, preds, average='macro', zero_division=0),\n",
        "        'recall': recall_score(labels, preds, average='macro', zero_division=0),\n",
        "        'f1': f1_value,\n",
        "    }\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "\n",
        "# Custom Trainer to ensure deterministic sampling\n",
        "class DeterministicTrainer(Trainer):\n",
        "    def get_train_dataloader(self):\n",
        "        generator = torch.Generator()\n",
        "        generator.manual_seed(self.args.seed)\n",
        "\n",
        "        sampler = RandomSampler(\n",
        "            self.train_dataset,\n",
        "            generator=generator\n",
        "        )\n",
        "\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.args.train_batch_size,\n",
        "            sampler=sampler,\n",
        "            num_workers=0,\n",
        "            drop_last=True,\n",
        "            collate_fn=self.data_collator,\n",
        "            pin_memory=self.args.dataloader_pin_memory,\n",
        "        )\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# Initialise Trainer\n",
        "trainer = DeterministicTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=val_tok,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce03dc79",
      "metadata": {
        "id": "ce03dc79"
      },
      "outputs": [],
      "source": [
        "from codecarbon import EmissionsTracker\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access CodeCarbon API Key and experiment ID in Google Colab\n",
        "api_key = userdata.get('CODECARBON_API_KEY')\n",
        "experiment_id = userdata.get('EXPERIMENT_ID')\n",
        "\n",
        "# Training with Carbon Emissions Tracking\n",
        "tracker = EmissionsTracker(\n",
        "    project_name='camembert-fairtranslate',\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    save_to_api=True,\n",
        "    api_endpoint = 'https://api.codecarbon.io',\n",
        "    experiment_id = experiment_id,\n",
        "    api_key = api_key,\n",
        "    api_call_interval = 1,\n",
        "    tracking_mode = 'process'\n",
        "    )\n",
        "tracker.start()\n",
        "trainer.train()\n",
        "tracker.stop()\n",
        "\n",
        "# Save model in directory\n",
        "trainer.save_model(os.path.join(OUTPUT_DIR, 'camembert_finetuned'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results\n",
        "\n",
        "| Epoch | Training Loss | Validation Loss | Precision | Recall | F1 |\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| 1 | No log | 0.552439 | 0.355372 | 0.500000 | 0.415459 |\n",
        "| 2 | 0.560849 | 0.359345 | 0.936548 | 0.821429 | 0.857429 |\n",
        "| 3 | 0.560849 | 0.177445 | 0.988636 | 0.971429 | 0.979547 |\n",
        "| 4 | 0.265455 | 0.119675 | 0.976837 | 0.972757 | 0.974768 |\n",
        "| 5 | 0.265455 | 0.100910 | 0.994253 | 0.985714 | 0.989863 |"
      ],
      "metadata": {
        "id": "2otZLT2EVUaB"
      },
      "id": "2otZLT2EVUaB"
    },
    {
      "cell_type": "markdown",
      "id": "04a47b58",
      "metadata": {
        "id": "04a47b58"
      },
      "source": [
        "## 8. Evaluation on test set\n",
        "\n",
        "**Metrics used:**\n",
        "- Classification report\n",
        "- Macro-F1\n",
        "- Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "692ef50d",
      "metadata": {
        "id": "692ef50d"
      },
      "outputs": [],
      "source": [
        "# Evaluate with trainer\n",
        "try:\n",
        "    camembert_logits = trainer.predict(test_tok).predictions\n",
        "    camembert_preds = np.argmax(camembert_logits, axis=-1)\n",
        "    print(\"CamemBERT Test classification report:\")\n",
        "    print(classification_report(y_test, camembert_preds, digits=4))\n",
        "    print(f\"Macro F1: {f1_score(y_test, camembert_preds, average='macro')}\")\n",
        "except Exception as e:\n",
        "    print(f\"No trained model available in this session. Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results\n",
        "\n",
        "**Macro F1**: 0.9873567842346413"
      ],
      "metadata": {
        "id": "pHXOX4i_YQ9E"
      },
      "id": "pHXOX4i_YQ9E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54A5kAd54YbJ",
      "metadata": {
        "id": "54A5kAd54YbJ"
      },
      "outputs": [],
      "source": [
        "# Model performance visualisations\n",
        "\n",
        "# Confusion matrix (CamemBERT)\n",
        "cm = confusion_matrix(y_test, camembert_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"CamemBERT – Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "# Comparison of macro-F1 scores (Lollipop plot)\n",
        "baseline_f1 = f1_score(y_test, baseline_preds, average=\"macro\")\n",
        "camembert_f1 = f1_score(y_test, camembert_preds, average=\"macro\")\n",
        "\n",
        "models = [\"TF-IDF + LR\", \"CamemBERT\"]\n",
        "scores = [baseline_f1, camembert_f1]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "\n",
        "plt.stem(models, scores)\n",
        "plt.scatter(models, scores, s=180)\n",
        "\n",
        "for i,v in enumerate(scores):\n",
        "    plt.text(i, v+0.002, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "plt.margins(x=0.6)\n",
        "plt.ylabel(\"Macro-F1\")\n",
        "plt.title(\"Performance Comparison\")\n",
        "plt.ylim(min(scores)-0.01, max(scores)+0.01)\n",
        "plt.show()\n",
        "\n",
        "# Comparison of macro-F1 scores (Table)\n",
        "print(\"\\nPerformance Comparison Table:\\n\")\n",
        "df = pd.DataFrame({\n",
        "    \"Model\": [\"TF-IDF + LR\", \"CamemBERT\"],\n",
        "    \"Macro-F1\": [baseline_f1, camembert_f1]\n",
        "})\n",
        "print(df)\n",
        "print()\n",
        "\n",
        "# Macro-F1 across epochs (CamemBERT)\n",
        "epochs = []\n",
        "f1_epochs = []\n",
        "\n",
        "for entry in trainer.state.log_history:\n",
        "    if \"eval_f1\" in entry:\n",
        "        epochs.append(entry[\"epoch\"])\n",
        "        f1_epochs.append(entry[\"eval_f1\"])\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, f1_epochs, marker='o', linewidth=2)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Macro-F1\")\n",
        "plt.title(\"Macro-F1 Across Training Epochs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e212a6",
      "metadata": {
        "id": "02e212a6"
      },
      "source": [
        "#### Results\n",
        "\n",
        "Performance Comparison Table:\n",
        "\n",
        "| | Model | Macro-F1 |\n",
        "| --- | --- | --- |\n",
        "| 0 | TF-IDF + LR | 0.985051 |\n",
        "| 1 | CamemBERT | 0.987357 |\n",
        "\n",
        "#### Interpretation\n",
        "\n",
        "These values indicate that the task is relatively easy for both traditional and transformer models, likely due to the strongly gender-marked morphology in French occupational terms, which provides explicit cues for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b41fc85c",
      "metadata": {
        "id": "b41fc85c"
      },
      "source": [
        "## 9. Statistical significance testing\n",
        "\n",
        "### McNemar's test (Baseline vs CamemBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54abead4",
      "metadata": {
        "id": "54abead4"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "\n",
        "# Build 2x2 contingency table:\n",
        "# b00 = both correct\n",
        "# b01 = baseline wrong, camembert correct\n",
        "# b10 = baseline correct, camembert wrong\n",
        "# b11 = both wrong\n",
        "\n",
        "b01 = np.sum((baseline_preds != y_test) & (camembert_preds == y_test))\n",
        "b10 = np.sum((baseline_preds == y_test) & (camembert_preds != y_test))\n",
        "\n",
        "table = [[0, b01],\n",
        "         [b10, 0]]\n",
        "\n",
        "result = mcnemar(table, exact=True)\n",
        "\n",
        "print(\"McNemar Test Results:\")\n",
        "print(f\"b01 (baseline wrong, camembert correct): {b01}\")\n",
        "print(f\"b10 (baseline correct, camembert wrong): {b10}\")\n",
        "print(f\"p-value: {result.pvalue:.6f}\")\n",
        "\n",
        "if result.pvalue < 0.05:\n",
        "    print(\"Result: Significant difference (p < 0.05). CamemBERT is meaningfully better.\")\n",
        "else:\n",
        "    print(\"Result: No significant difference (p >= 0.05).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b047a5e",
      "metadata": {
        "id": "9b047a5e"
      },
      "source": [
        "#### Results\n",
        "\n",
        "- **b01** (baseline wrong, camembert correct): 4\n",
        "- **b10** (baseline correct, camembert wrong): 3\n",
        "- **p-value**: 1.000000\n",
        "\n",
        "**Result**: No significant difference (p >= 0.05).\n",
        "\n",
        "#### Interpretation\n",
        "\n",
        "There is no statistically significant difference between the two models (p ≥ 0.05). Although CamemBERT performs slightly better in absolute terms, the small number of disagreements makes it statistically indistinguishable from the TF-IDF baseline.\n",
        "\n",
        "This differs from the HEARTS results, where BERT models significantly outperformed the baseline. The difference likely arises from:\n",
        "- The small dataset size (2.4k examples in FairTranslate vs 30k+ in EMGSD)\n",
        "- French encodes gender explicitly\n",
        "- The limited complexity of the sentences\n",
        "- Strong surface-level features that TF-IDF can capture without deep semantic modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac5415f4",
      "metadata": {
        "id": "ac5415f4"
      },
      "source": [
        "### Bootstrap confidence intervals for Macro-F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e73231",
      "metadata": {
        "id": "15e73231"
      },
      "outputs": [],
      "source": [
        "def bootstrap_f1(preds, labels, B=2000):\n",
        "    n = len(labels)\n",
        "    scores = []\n",
        "    for _ in range(B):\n",
        "        idx = np.random.choice(n, n, replace=True)\n",
        "        scores.append(f1_score(labels[idx], preds[idx], average=\"macro\"))\n",
        "    return np.percentile(scores, [2.5, 50, 97.5])\n",
        "\n",
        "print(\"Bootstrap Confidence Intervals:\")\n",
        "\n",
        "base_ci = bootstrap_f1(baseline_preds, y_test.values)\n",
        "cam_ci = bootstrap_f1(camembert_preds, y_test.values)\n",
        "\n",
        "print(f\"Baseline Macro-F1: 95% CI = [{base_ci[0]:.4f}, {base_ci[2]:.4f}]  (median={base_ci[1]:.4f})\")\n",
        "print(f\"CamemBERT Macro-F1: 95% CI = [{cam_ci[0]:.4f}, {cam_ci[2]:.4f}]  (median={cam_ci[1]:.4f})\")\n",
        "\n",
        "# Confidence intervals\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.errorbar(\n",
        "    x=[\"Baseline\", \"CamemBERT\"],\n",
        "    y=[base_ci[1], cam_ci[1]],  # median\n",
        "    yerr=[[base_ci[1]-base_ci[0], cam_ci[1]-cam_ci[0]],\n",
        "          [base_ci[2]-base_ci[1], cam_ci[2]-cam_ci[1]]],\n",
        "    fmt='o',\n",
        "    capsize=5\n",
        ")\n",
        "plt.margins(x=0.6)\n",
        "plt.title(\"95% Bootstrap CI for Macro-F1\")\n",
        "plt.ylabel(\"Macro-F1\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "034732af",
      "metadata": {
        "id": "034732af"
      },
      "source": [
        "#### Results\n",
        "\n",
        "- **Baseline Macro-F1**: 95% CI = [0.9717, 0.9951] (median=0.9855)\n",
        "- **CamemBERT Macro-F1**: 95% CI = [0.9750, 0.9974] (median=0.9875)\n",
        "\n",
        "#### Interpretation\n",
        "\n",
        "The overlapping ranges indicate that both models achieve consistently high performance across resampled subsets, with only minimal variation.\n",
        "CamemBERT has a slightly higher median Macro-F1 (0.9876 vs 0.9855), but the strong overlap between the two intervals confirms that this difference is not statistically meaningful."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156caa02",
      "metadata": {
        "id": "156caa02"
      },
      "source": [
        "### Qualitative error analysis (CamemBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e52c377",
      "metadata": {
        "id": "3e52c377"
      },
      "outputs": [],
      "source": [
        "error_df = test_df.copy()\n",
        "error_df['pred'] = camembert_preds\n",
        "error_df['correct'] = (error_df['pred'] == error_df['label'])\n",
        "\n",
        "# Extract top 3 most interesting failures\n",
        "failures = error_df[~error_df['correct']].sample(n=min(3, len(error_df)), random_state=SEED)\n",
        "\n",
        "for i, row in failures.iterrows():\n",
        "    print(f\"French text: {row['french']}\")\n",
        "    print(f\"True label: {row['label']}\")\n",
        "    print(f\"Predicted: {row['pred']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d30d7ae4",
      "metadata": {
        "id": "d30d7ae4"
      },
      "source": [
        "#### Failure case interpretation\n",
        "The failure cases show that CamemBERT:\n",
        "- Overweights morphological gender markers\n",
        "- Associates any gendered job title with a stereotype, regardless of dataset label\n",
        "- Underutilises context, particularly in occupations not strongly associated with gender in the real-world statistics used by FairTranslate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb5aa464",
      "metadata": {
        "id": "fb5aa464"
      },
      "source": [
        "## 10. Save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2DeQriGuWr44",
      "metadata": {
        "id": "2DeQriGuWr44"
      },
      "outputs": [],
      "source": [
        "# Access Google Drive to save output folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!cp -r outputs_camembert_fairtranslate/ drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a403c9e3",
      "metadata": {
        "id": "a403c9e3"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "#### Comparison to HEARTS\n",
        "The HEARTS study reports that ALBERT-V2 substantially outperforms a TF-IDF+LR baseline, with macro-F1 improvements of approximately 12-15 points.\n",
        "\n",
        "In contrast, in the French adaptation:\n",
        "- Both models achieve extremely high Macro-F1\n",
        "- The performance gap is very small (0.987 vs 0.985)\n",
        "- McNemar’s test shows that CamemBERT does not significantly outperform the baseline (p = 1.0)\n",
        "\n",
        "This divergence might arise from structural differences between English and French:\n",
        "\n",
        "- English encodes gender implicitly (“he”, “she”, “they”), making stereotype classification harder\n",
        "- French uses explicit gender morphology, which reduces the complexity of the classification task\n",
        "- TF-IDF captures these explicit surface features almost as effectively as a transformer\n",
        "- The dataset is much smaller than EMGSD, which reducesthe potential for deeper contextual modeling advantages\n",
        "\n",
        "The table below shows how the original HEARTS Transformer models perform compared to this study:\n",
        "\n",
        "| Project | Model Type | Emissions | Training Data | Test Set Macro F1 Score |\n",
        "| ------- | ---------- | --------- | ------------- | ----------------------- |\n",
        "| HEARTS | ALBERT-V2 | 2.88g | EMGSD | 81.5% |\n",
        "| HEARTS | DistilBERT | 156.48g | EMGSD | 80.6% |\n",
        "| HEARTS | BERT | 270.68g | EMGSD | 82.8% |\n",
        "| French Adaptation | CamemBERT | 2.39g | FairTranslate | 98.76% |\n",
        "\n",
        "#### Limitations\n",
        "- Small dataset restricts generalisation\n",
        "- No SHAP/LIME due to instability, replaced with statistical tests\n",
        "- French bias patterns differ structurally from English, limiting direct comparability\n",
        "\n",
        "#### Future Work\n",
        "- Expand dataset with crowd-sourced French occupational sentences\n",
        "- Include contextual embeddings for long-range gender cues\n",
        "- Evaluate robustness across multiple French dialects"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
