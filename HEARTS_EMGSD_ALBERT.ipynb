{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cec70/HEARTS-French-Adaptation/blob/main/HEARTS_EMGSD_ALBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWe8hqGN19WM"
      },
      "source": [
        "# HEARTS Baseline: ALBERT-V2 (EMGSD Dataset Only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFDKQUWfhfT1"
      },
      "source": [
        "## 0. Installments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpz2CPlChfT1"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/holistic-ai/HEARTS-Text-Stereotype-Detection.git\n",
        "%cd HEARTS-Text-Stereotype-Detection\n",
        "!pip install codecarbon --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OHXOqNn2ngT"
      },
      "source": [
        "## 1. Setup and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJUa5HYn0CFI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support, balanced_accuracy_score\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
        "\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPRNWn3U3Kcy"
      },
      "source": [
        "## 2. Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-N7Sg873QcG"
      },
      "outputs": [],
      "source": [
        "def data_loader(csv_file_path, labelling_criteria, dataset_name, sample_size=1000000, num_examples=5):\n",
        "    \"\"\"\n",
        "    Loads one dataset, applies binary labeling, samples if needed,\n",
        "    and creates train/test split.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(csv_file_path, usecols=['text', 'label', 'group'])\n",
        "\n",
        "    # Binary label mapping\n",
        "    label2id = {label: (1 if label == labelling_criteria else 0)\n",
        "                for label in df['label'].unique()}\n",
        "    df['label'] = df['label'].map(label2id)\n",
        "\n",
        "    df['data_name'] = dataset_name\n",
        "\n",
        "    # Sampling\n",
        "    if sample_size < len(df):\n",
        "        df, _ = train_test_split(\n",
        "            df,\n",
        "            train_size=sample_size / len(df),\n",
        "            stratify=df['label'],\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # Train/test split\n",
        "    train_df, test_df = train_test_split(\n",
        "        df,\n",
        "        test_size=0.2,\n",
        "        stratify=df['label'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\nLoaded {dataset_name}\")\n",
        "    print(f\"Train size: {len(train_df)}\")\n",
        "    print(f\"Test size: {len(test_df)}\\n\")\n",
        "\n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHFAkbV-3TDQ"
      },
      "source": [
        "## 3. Merge datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFn_Xh073XCi"
      },
      "outputs": [],
      "source": [
        "def merge_datasets(train_a, test_a, train_b, test_b, num_examples=5):\n",
        "    merged_train = pd.concat([train_a, train_b], ignore_index=True)\n",
        "    merged_test = pd.concat([test_a, test_b], ignore_index=True)\n",
        "\n",
        "    print(\"\\nMerged Dataset:\")\n",
        "    print(f\"Train size: {len(merged_train)}\")\n",
        "    print(f\"Test size: {len(merged_test)}\\n\")\n",
        "\n",
        "    return merged_train, merged_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oof-Lx8E3YwY"
      },
      "source": [
        "## 4. Training function (ALBERT-V2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjzwF5oG3ePb"
      },
      "outputs": [],
      "source": [
        "def train_model(train_df, model_path, batch_size, epochs, lr, output_base, dataset_name, seed):\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    num_labels = len(train_df['label'].unique())\n",
        "\n",
        "    print(f\"\\nNumber of labels: {num_labels}\")\n",
        "\n",
        "    tracker = EmissionsTracker()\n",
        "    tracker.start()\n",
        "\n",
        "    # Load model + tokenizer\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_path,\n",
        "        num_labels=num_labels,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    # Special GPT fix\n",
        "    if model_path.startswith(\"gpt\"):\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Train/val split\n",
        "    train_df, val_df = train_test_split(\n",
        "        train_df,\n",
        "        test_size=0.2,\n",
        "        stratify=train_df['label'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_ds = Dataset.from_pandas(train_df).map(tokenize, batched=True)\n",
        "    train_ds = train_ds.map(lambda x: {\"labels\": x[\"label\"]})\n",
        "\n",
        "    val_ds = Dataset.from_pandas(val_df).map(tokenize, batched=True)\n",
        "    val_ds = val_ds.map(lambda x: {\"labels\": x[\"label\"]})\n",
        "\n",
        "    # Metrics\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=1)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            labels, preds, average='macro'\n",
        "        )\n",
        "        bal_acc = balanced_accuracy_score(labels, preds)\n",
        "        return {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"balanced_accuracy\": bal_acc\n",
        "        }\n",
        "\n",
        "    # Output folder\n",
        "    output_dir = os.path.join(output_base, dataset_name)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=lr,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit=1\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(output_dir)\n",
        "\n",
        "    emissions = tracker.stop()\n",
        "    print(f\"\\nEstimated emissions: {emissions:.4f} kg\")\n",
        "\n",
        "    return output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6O3oiAA3iVg"
      },
      "source": [
        "## 5. Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3dEULwH3om1"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(test_df, model_dir, results_base, dataset_name, seed):\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    num_labels = len(test_df['label'].unique())\n",
        "    print(f\"\\nEvaluating - Number of labels: {num_labels}\")\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_dir,\n",
        "        num_labels=num_labels,\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Tokenize for pipeline\n",
        "    pipe = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=-1\n",
        "    )\n",
        "\n",
        "    # Predictions\n",
        "    predictions = pipe(test_df['text'].tolist(), return_all_scores=True)\n",
        "    pred_labels = [int(max(p, key=lambda x: x['score'])['label'].split('_')[-1])\n",
        "                   for p in predictions]\n",
        "    pred_probs = [max(p, key=lambda x: x['score'])['score']\n",
        "                  for p in predictions]\n",
        "\n",
        "    # Save results\n",
        "    out_dir = os.path.join(results_base, dataset_name)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    results = pd.DataFrame({\n",
        "        \"text\": test_df['text'],\n",
        "        \"predicted_label\": pred_labels,\n",
        "        \"predicted_probability\": pred_probs,\n",
        "        \"actual_label\": test_df['label'],\n",
        "        \"group\": test_df['group'],\n",
        "        \"dataset_name\": test_df['data_name']\n",
        "    })\n",
        "\n",
        "    results.to_csv(os.path.join(out_dir, \"full_results.csv\"), index=False)\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(test_df['label'], pred_labels, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df.to_csv(os.path.join(out_dir, \"classification_report.csv\"))\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    display(report_df)\n",
        "\n",
        "    return report_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwa4L0lV3rQM"
      },
      "source": [
        "## 6. Load and merge datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSvPoSIm3u9d"
      },
      "outputs": [],
      "source": [
        "train_wq, test_wq = data_loader(\n",
        "    \"Model Training and Evaluation/Winoqueer - GPT Augmentation.csv\",\n",
        "    labelling_criteria=\"stereotype\",\n",
        "    dataset_name=\"WinoQueer\"\n",
        ")\n",
        "\n",
        "train_sg, test_sg = data_loader(\n",
        "    \"Model Training and Evaluation/SeeGULL - GPT Augmentation.csv\",\n",
        "    labelling_criteria=\"stereotype\",\n",
        "    dataset_name=\"SeeGULL\"\n",
        ")\n",
        "\n",
        "train_mgsd, test_mgsd = data_loader(\n",
        "    \"Model Training and Evaluation/MGSD.csv\",\n",
        "    labelling_criteria=\"stereotype\",\n",
        "    dataset_name=\"MGSD\"\n",
        ")\n",
        "\n",
        "# Merge WinoQueer + SeeGULL\n",
        "train_wq_sg, test_wq_sg = merge_datasets(train_wq, test_wq, train_sg, test_sg)\n",
        "\n",
        "# Merge (WQ + SG) with MGSD\n",
        "train_merged, test_merged = merge_datasets(train_wq_sg, test_wq_sg, train_mgsd, test_mgsd)\n",
        "\n",
        "print(\"\\nFinal merged dataset shapes:\")\n",
        "print(\"Train:\", train_merged.shape)\n",
        "print(\"Test:\", test_merged.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O2ObPCh3_qO"
      },
      "source": [
        "## 7. Train ALBERT-V2 on the merged dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr4V54YG4YWc"
      },
      "outputs": [],
      "source": [
        "model_output_dir = train_model(\n",
        "    train_df=train_merged,\n",
        "    model_path=\"albert/albert-base-v2\",\n",
        "    batch_size=64,\n",
        "    epochs=6,\n",
        "    lr=2e-5,\n",
        "    output_base=\"model_output_albert_baseline\",\n",
        "    dataset_name=\"merged_wq_seegull_mgsd\",\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results\n",
        "\n",
        "| Epoch\t| Training Loss\t| Validation Loss\t| Precision\t| Recall | F1\t|Balanced Accuracy |\n",
        "| --- | --- | --- | --- | --- | --- | --- |\n",
        "| 1\t| 0.469300 | 0.415653\t| 0.768429 | 0.795270 | 0.772058 | 0.795270 |\n",
        "| 2\t| 0.345400 | 0.365308\t| 0.810502 | 0.817595 | 0.813748 | 0.817595 |\n",
        "| 3\t| 0.258900 | 0.394409\t| 0.812467 | 0.824586 | 0.817594 | 0.824586 |\n",
        "| 4\t| 0.186800 | 0.458717\t| 0.815537 | 0.818966 | 0.817186 | 0.818966 |\n",
        "| 5\t| 0.114500 | 0.574235\t| 0.817568 | 0.816777 | 0.817169 | 0.816777 |\n",
        "| 6\t| 0.061600 | 0.732173\t| 0.817217 | 0.814263 | 0.815695 | 0.814263 |\n",
        "\n",
        "Estimated emissions: 0.0318 kg\n"
      ],
      "metadata": {
        "id": "i5EGk4Mn2THH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afRUy95S4bLL"
      },
      "source": [
        "## 8. Evaluation on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZoNT9Ub4loq"
      },
      "outputs": [],
      "source": [
        "results = evaluate_model(\n",
        "    test_df=test_merged,\n",
        "    model_dir=model_output_dir,\n",
        "    results_base=\"results_albert_baseline\",\n",
        "    dataset_name=\"merged_wq_seegull_mgsd\",\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results\n",
        "\n",
        "Classification report:\n",
        "\n",
        "|index|precision|recall|f1-score|support|\n",
        "|---|---|---|---|---|\n",
        "|0|0\\.8779|0\\.8601|0\\.8689|7540\\.0|\n",
        "|1|0\\.7398|0\\.7688|0\\.7540|3901\\.0|\n",
        "|accuracy|0\\.8289|0\\.8289|0\\.8289|0\\.8289|\n",
        "|macro avg|0\\.8088|0\\.8144|0\\.8114|11441\\.0|\n",
        "|weighted avg|0\\.8308|0\\.8289|0\\.8297|11441\\.0|\n",
        "\n",
        "Macro F1: **81.14%**"
      ],
      "metadata": {
        "id": "raML8WkF1Jkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Discussion & Conclusion\n",
        "\n",
        "The ALBERT baseline trained and tested on the EMGSD dataset achieved a macro-F1 score of **81.14%**, which is highly consistent with the **81.5%** macro-F1 reported in the original HEARTS paper. The difference of **0.36%** is well within the requirement of reproducing results within **Â±5%** of the paper's baseline performance.\n",
        "\n",
        "Overall, the successful replication of the baseline demonstrates both the stability of the initial approach and the robustness of the pipeline implemented."
      ],
      "metadata": {
        "id": "r800AAdZ37gL"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}